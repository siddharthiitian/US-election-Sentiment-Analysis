{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy regex textblob transformers torch scikit-learn tensorflow setuptools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_handle</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>candidate</th>\n",
       "      <th>party</th>\n",
       "      <th>retweets</th>\n",
       "      <th>likes</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@user123</td>\n",
       "      <td>2024-11-03 08:45:00</td>\n",
       "      <td>Excited to see Kamala Harris leading the Democ...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Democratic Party</td>\n",
       "      <td>120</td>\n",
       "      <td>450</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@politicsFan</td>\n",
       "      <td>2024-11-03 09:15:23</td>\n",
       "      <td>Donald Trump's policies are the best for our e...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Republican Party</td>\n",
       "      <td>85</td>\n",
       "      <td>300</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>@greenAdvocate</td>\n",
       "      <td>2024-11-03 10:05:45</td>\n",
       "      <td>Jill Stein's environmental plans are exactly w...</td>\n",
       "      <td>Jill Stein</td>\n",
       "      <td>Green Party</td>\n",
       "      <td>60</td>\n",
       "      <td>200</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@indieVoice</td>\n",
       "      <td>2024-11-03 11:20:10</td>\n",
       "      <td>Robert Kennedy offers a fresh perspective outs...</td>\n",
       "      <td>Robert Kennedy</td>\n",
       "      <td>Independent</td>\n",
       "      <td>40</td>\n",
       "      <td>150</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@libertyLover</td>\n",
       "      <td>2024-11-03 12:35:55</td>\n",
       "      <td>Chase Oliver's libertarian stance promotes tru...</td>\n",
       "      <td>Chase Oliver</td>\n",
       "      <td>Libertarian Party</td>\n",
       "      <td>30</td>\n",
       "      <td>120</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id     user_handle            timestamp  \\\n",
       "0         1        @user123  2024-11-03 08:45:00   \n",
       "1         2    @politicsFan  2024-11-03 09:15:23   \n",
       "2         3  @greenAdvocate  2024-11-03 10:05:45   \n",
       "3         4     @indieVoice  2024-11-03 11:20:10   \n",
       "4         5   @libertyLover  2024-11-03 12:35:55   \n",
       "\n",
       "                                          tweet_text       candidate  \\\n",
       "0  Excited to see Kamala Harris leading the Democ...   Kamala Harris   \n",
       "1  Donald Trump's policies are the best for our e...    Donald Trump   \n",
       "2  Jill Stein's environmental plans are exactly w...      Jill Stein   \n",
       "3  Robert Kennedy offers a fresh perspective outs...  Robert Kennedy   \n",
       "4  Chase Oliver's libertarian stance promotes tru...    Chase Oliver   \n",
       "\n",
       "               party  retweets  likes sentiment  \n",
       "0   Democratic Party       120    450  positive  \n",
       "1   Republican Party        85    300  positive  \n",
       "2        Green Party        60    200  positive  \n",
       "3        Independent        40    150   neutral  \n",
       "4  Libertarian Party        30    120  positive  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HMTL Tages, lowercase, remove @mentions\n",
    "clean = re.compile('<.*?>')\n",
    "train_csv['tweet_text'] = train_csv['tweet_text'].str.lower()\n",
    "train_csv['tweet_text'] = train_csv['tweet_text'].apply(lambda x: re.sub(clean, '',x) )\n",
    "train_csv['tweet_text'] = train_csv['tweet_text'].str.replace(r'@\\S+', '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove Puncuations\n",
    "train_csv['tweet_text'] = train_csv['tweet_text'].apply(lambda x : x.translate(str.maketrans('', '',string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spelling corrections\n",
    "from textblob import TextBlob\n",
    "train_csv['tweet_text'] = train_csv['tweet_text'].apply(lambda x: TextBlob(x).correct().string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train_csv['tweet_text'][0]\n",
    "\n",
    "encoded_input = tokenizer(text,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['pooler_output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Word Embeddings: torch.Size([1, 11, 768])\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings using BERT model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    word_embeddings = outputs.last_hidden_state  # This contains the embeddings\n",
    "\n",
    "# Output the shape of word embeddings\n",
    "print(f'Shape of Word Embeddings: {word_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text: excited to see mala harris leading the democratic charge\n",
      "tokenized Text: ['excited', 'to', 'see', 'mala', 'harris', 'leading', 'the', 'democratic', 'charge']\n",
      "Encoded Text: tensor([[  101,  7568,  2000,  2156, 28935,  5671,  2877,  1996,  3537,  3715,\n",
      "           102]])\n"
     ]
    }
   ],
   "source": [
    "# Decode the token IDs back to text\n",
    "decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "#print decoded text\n",
    "print(f'Decoded Text: {decoded_text}')\n",
    "# Tokenize the text again for reference\n",
    "tokenized_text = tokenizer.tokenize(decoded_text)\n",
    "#print tokenized text\n",
    "print(f'tokenized Text: {tokenized_text}')\n",
    "# Encode the text\n",
    "encoded_text = tokenizer.encode(text, return_tensors='pt')  # Returns a tensor\n",
    "# Print encoded text\n",
    "print(f'Encoded Text: {encoded_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1750,  0.0534, -0.1162,  ..., -0.0963,  0.2406,  0.3589],\n",
       "         [ 0.6159, -0.6472, -0.3491,  ..., -0.1919,  0.4275,  0.5265],\n",
       "         [ 0.2682, -0.4770,  0.1700,  ..., -0.3569,  0.2511,  0.0313],\n",
       "         ...,\n",
       "         [-0.4254, -0.6637, -0.1005,  ..., -0.3504, -0.3146,  0.5374],\n",
       "         [-0.5015, -0.9129, -0.1265,  ...,  0.1249,  0.0892, -0.3274],\n",
       "         [ 0.6415,  0.0750, -0.3870,  ...,  0.1416, -0.6869, -0.0764]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embeddings_BERT(text):\n",
    "    text = train_csv['tweet_text'][0]\n",
    "\n",
    "    encoding = tokenizer.batch_encode_plus( [text],# List of input texts\n",
    "        padding=True,              # Pad to the maximum sequence length\n",
    "        truncation=True,           # Truncate to the maximum sequence length if necessary\n",
    "        return_tensors='pt',      # Return PyTorch tensors\n",
    "        add_special_tokens=True    # Add special tokens CLS and SEP\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids']  # Token IDs\n",
    "    # print input IDs\n",
    "    # print(f'Input ID: {input_ids}')\n",
    "    attention_mask = encoding['attention_mask']  # Attention mask\n",
    "    # print attention mask\n",
    "    # print(f'Attention mask: {attention_mask}')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        word_embeddings = outputs.last_hidden_state  # This contains the embeddings\n",
    "\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 768])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings_BERT(train_csv['tweet_text'][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv['tweet_text_embeded'] = train_csv['tweet_text'].apply(lambda x: word_embeddings_BERT(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv.to_csv('Data/train_embeded.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cls_or_mean(embedding, method=\"cls\"):\n",
    "    \"\"\"\n",
    "    Extract sentence representation using CLS token or mean pooling.\n",
    "    \"\"\"\n",
    "    # Remove batch dimension: Shape [1, sequence_length, 768] -> [sequence_length, 768]\n",
    "    embedding = embedding.squeeze(0)\n",
    "    \n",
    "    if method == \"cls\":\n",
    "        return embedding[0]  # CLS token embedding\n",
    "    elif method == \"mean\":\n",
    "        return embedding.mean(dim=0)  # Mean pooling\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported pooling method. Choose 'cls' or 'mean'.\")\n",
    "    \n",
    "train_csv[\"sentence_embedding\"] = train_csv[\"tweet_text_embeded\"].apply(lambda x: extract_cls_or_mean(x, method=\"cls\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv['sentence_embedding'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive      328\n",
       "neutral       126\n",
       "negative       45\n",
       "positive        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "train_csv[\"sentimen\"] = train_csv[\"sentiment\"].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "# Prepare dataset\n",
    "embeddings = torch.stack(train_csv[\"sentence_embedding\"].tolist())  # Shape: [num_samples, 768]\n",
    "labels = torch.tensor(train_csv[\"sentimen\"].tolist())  # Shape: [num_samples]\n",
    "\n",
    "dataset = SentimentDataset(embeddings, labels)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.fc(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 29.0489\n",
      "Epoch 2, Loss: 29.0405\n",
      "Epoch 3, Loss: 29.0319\n",
      "Epoch 4, Loss: 28.6610\n",
      "Epoch 5, Loss: 28.4721\n",
      "Epoch 6, Loss: 28.6467\n",
      "Epoch 7, Loss: 29.1902\n",
      "Epoch 8, Loss: 28.8197\n",
      "Epoch 9, Loss: 28.6333\n",
      "Epoch 10, Loss: 28.6300\n",
      "Epoch 11, Loss: 28.6260\n",
      "Epoch 12, Loss: 28.6238\n",
      "Epoch 13, Loss: 28.8057\n",
      "Epoch 14, Loss: 28.6181\n",
      "Epoch 15, Loss: 28.9848\n",
      "Epoch 16, Loss: 28.6142\n",
      "Epoch 17, Loss: 28.6124\n",
      "Epoch 18, Loss: 28.6113\n",
      "Epoch 19, Loss: 28.9793\n",
      "Epoch 20, Loss: 28.4230\n",
      "Epoch 21, Loss: 28.6068\n",
      "Epoch 22, Loss: 28.4202\n",
      "Epoch 23, Loss: 28.6044\n",
      "Epoch 24, Loss: 28.6034\n",
      "Epoch 25, Loss: 28.6029\n",
      "Epoch 26, Loss: 28.4159\n",
      "Epoch 27, Loss: 28.6008\n",
      "Epoch 28, Loss: 28.4142\n",
      "Epoch 29, Loss: 28.5993\n",
      "Epoch 30, Loss: 28.5989\n",
      "Epoch 31, Loss: 28.5980\n",
      "Epoch 32, Loss: 28.4113\n",
      "Epoch 33, Loss: 28.7830\n",
      "Epoch 34, Loss: 28.7826\n",
      "Epoch 35, Loss: 28.4097\n",
      "Epoch 36, Loss: 28.5958\n",
      "Epoch 37, Loss: 28.9680\n",
      "Epoch 38, Loss: 28.7813\n",
      "Epoch 39, Loss: 28.4079\n",
      "Epoch 40, Loss: 28.5942\n",
      "Epoch 41, Loss: 28.4071\n",
      "Epoch 42, Loss: 29.1532\n",
      "Epoch 43, Loss: 28.5930\n",
      "Epoch 44, Loss: 28.4061\n",
      "Epoch 45, Loss: 28.5926\n",
      "Epoch 46, Loss: 28.9653\n",
      "Epoch 47, Loss: 28.5920\n",
      "Epoch 48, Loss: 28.5916\n",
      "Epoch 49, Loss: 28.4047\n",
      "Epoch 50, Loss: 28.4044\n",
      "Epoch 51, Loss: 28.7778\n",
      "Epoch 52, Loss: 28.9642\n",
      "Epoch 53, Loss: 28.7773\n",
      "Epoch 54, Loss: 28.9642\n",
      "Epoch 55, Loss: 28.5903\n",
      "Epoch 56, Loss: 28.9636\n",
      "Epoch 57, Loss: 28.5898\n",
      "Epoch 58, Loss: 28.7767\n",
      "Epoch 59, Loss: 28.9634\n",
      "Epoch 60, Loss: 28.5894\n",
      "Epoch 61, Loss: 28.4023\n",
      "Epoch 62, Loss: 28.7760\n",
      "Epoch 63, Loss: 28.5889\n",
      "Epoch 64, Loss: 28.4019\n",
      "Epoch 65, Loss: 28.4017\n",
      "Epoch 66, Loss: 28.5885\n",
      "Epoch 67, Loss: 28.4014\n",
      "Epoch 68, Loss: 28.7752\n",
      "Epoch 69, Loss: 28.9622\n",
      "Epoch 70, Loss: 28.5882\n",
      "Epoch 71, Loss: 28.5879\n",
      "Epoch 72, Loss: 28.7750\n",
      "Epoch 73, Loss: 28.5877\n",
      "Epoch 74, Loss: 28.7748\n",
      "Epoch 75, Loss: 28.9619\n",
      "Epoch 76, Loss: 28.5875\n",
      "Epoch 77, Loss: 28.7745\n",
      "Epoch 78, Loss: 28.7744\n",
      "Epoch 79, Loss: 28.5872\n",
      "Epoch 80, Loss: 28.5872\n",
      "Epoch 81, Loss: 28.5872\n",
      "Epoch 82, Loss: 28.7741\n",
      "Epoch 83, Loss: 28.3998\n",
      "Epoch 84, Loss: 28.9612\n",
      "Epoch 85, Loss: 28.3996\n",
      "Epoch 86, Loss: 28.9612\n",
      "Epoch 87, Loss: 28.5867\n",
      "Epoch 88, Loss: 28.7739\n",
      "Epoch 89, Loss: 28.7738\n",
      "Epoch 90, Loss: 28.9609\n",
      "Epoch 91, Loss: 28.7736\n",
      "Epoch 92, Loss: 28.7736\n",
      "Epoch 93, Loss: 28.7735\n",
      "Epoch 94, Loss: 28.5863\n",
      "Epoch 95, Loss: 28.7734\n",
      "Epoch 96, Loss: 28.5862\n",
      "Epoch 97, Loss: 28.3989\n",
      "Epoch 98, Loss: 28.7734\n",
      "Epoch 99, Loss: 28.5860\n",
      "Epoch 100, Loss: 28.5860\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_embeddings, batch_labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ensure batch_labels are of type LongTensor\n",
    "        batch_labels = batch_labels.long()\n",
    "        \n",
    "        outputs = model(batch_embeddings)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddharth/Documents/US-Election-Sentiment/lib/python3.12/site-packages/sklearn/utils/_array_api.py:392: RuntimeWarning: invalid value encountered in cast\n",
      "  return x.astype(dtype, copy=copy, casting=casting)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y_true contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m         predictions\u001b[38;5;241m.\u001b[39mextend(preds\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     10\u001b[0m         actuals\u001b[38;5;241m.\u001b[39mextend(batch_labels\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m---> 12\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactuals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/US-Election-Sentiment/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/US-Election-Sentiment/lib/python3.12/site-packages/sklearn/metrics/_classification.py:227\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m    226\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n\u001b[0;32m--> 227\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/US-Election-Sentiment/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     97\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m     98\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m---> 99\u001b[0m type_true \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[0;32m~/Documents/US-Election-Sentiment/lib/python3.12/site-packages/sklearn/utils/multiclass.py:417\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name, raise_unknown)\u001b[0m\n\u001b[1;32m    415\u001b[0m     data \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m issparse(y) \u001b[38;5;28;01melse\u001b[39;00m y\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39many(data \u001b[38;5;241m!=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(data, \u001b[38;5;28mint\u001b[39m)):\n\u001b[0;32m--> 417\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# Check multiclass\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/US-Election-Sentiment/lib/python3.12/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/US-Election-Sentiment/lib/python3.12/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input y_true contains NaN."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model.eval()\n",
    "predictions, actuals = [], []\n",
    "with torch.no_grad():\n",
    "    for batch_embeddings, batch_labels in train_loader:  # Use test_loader for evaluation\n",
    "        outputs = model(batch_embeddings)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        predictions.extend(preds.tolist())\n",
    "        actuals.extend(batch_labels.tolist())\n",
    "\n",
    "accuracy = accuracy_score(actuals, predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in ./lib/python3.12/site-packages (75.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtext\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/US-Election-Sentiment/lib/python3.12/site-packages/tensorflow/__init__.py:30\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mTop-level module of TensorFlow. By convention, we refer to this module as\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m`tf` instead of `tensorflow`, following the common practice of importing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03mthis file with a file generated from [`api_template.__init__.py`](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/api_template.__init__.py)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order,protected-access,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_distutils\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_inspect\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "US-Election-Sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
